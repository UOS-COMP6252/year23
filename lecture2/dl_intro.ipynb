{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/pytorch/blob/main/dl_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXG2Gc34Vqio",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What you will learn in this notebook\n",
    " \n",
    "1. Introduction to supervised learning and classification\n",
    "1. Introduction to loss functions, activation functions, and gradient descent\n",
    "1. Introduction to Pytorch packages, tensors, computation graphs, and gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpJEiPpbHjjk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CIFAR10 Dataset\n",
    "\n",
    "- We use the CIFAR10 dataset to perform binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4j6mq7QsZzsF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision as vision\n",
    "\n",
    "cifar10_train=vision.datasets.CIFAR10(\".\",download=True,train=True)# train=True is the default\n",
    "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7zBqVB3oCeV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data transforms\n",
    "- Recall from last session that CIFAR10 contains a set of images/labels. To use the dataset with PyTorch we need to **transform** the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JziiKBlKnDqH",
    "outputId": "e3603c30-1504-430c-b021-6e9732b0d4b6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "itr=iter(cifar10_train)\n",
    "img,label=next(itr)\n",
    "print(type(img),type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "cifar10_train=vision.datasets.CIFAR10(\".\",download=True,train=True,transform=ToTensor())\n",
    "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False,transform=ToTensor())\n",
    "itr=iter(cifar10_train)\n",
    "img,label=next(itr)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary classification\n",
    "\n",
    "- The dataset has 10 classes: Airplanes,Cars,Birds,Cats,Deers,Dogs,Frogs,Horses, Ships and Trucks\n",
    "- For simplicity we will rearrange it into only 2 classes\n",
    "    - Animate: Birds, Cats,Deers,Dogs, Frogs, Horses\n",
    "    - Inanimate\": Airplanes, Cars, Shipts and Trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Rearrange both the train and test datasets\n",
    "- Why didn't we use ```label=1``` and ```label=0``` in the code below ?\n",
    "- Use Vevox session 131-699-292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#airplane=0,car=1,bird=2,cat=3,deer=4,dog=5,frog=6,horse=7,ship=8,truck=9\n",
    "features=torch.tensor([0,1,8,9])\n",
    "for i, (img,label) in enumerate(cifar10_train):\n",
    "    if torch.isin(label,features):\n",
    "        cifar10_train.targets[i]=1\n",
    "    else:\n",
    "        cifar10_train.targets[i]=0\n",
    "for i, (img,label) in enumerate(cifar10_test):\n",
    "    if torch.isin(label,features):\n",
    "        cifar10_test.targets[i]=1\n",
    "    else:\n",
    "        cifar10_test.targets[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- So far the gradient was computed over the whole dataset\n",
    "    - In many situations this is not feasible, e.g. not enough memory\n",
    "- A good approximation is stochastic gradient descent\n",
    "    - The gradient is computed for a single sample\n",
    "- Another, most commonly used variant, is to compute the gradient over a random subset of the dataset (batch)\n",
    "    - Saves memory\n",
    "    - Better chance to escape local minima\n",
    "- We will refer to the above version as stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data loader\n",
    "- SGD is more efficient when the batches are randomly selected\n",
    "- PyTorch provides a convenient class for operations on batches: ```DataLoader```\n",
    "- ```num_workers``` is the number of threads used for parallel processing\n",
    "- ```shuffle=True``` means the dataset is randomly shuffled after every pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=64\n",
    "train_loader=DataLoader(cifar10_train,shuffle=True,batch_size=batch_size,num_workers=2)\n",
    "test_loader=DataLoader(cifar10_test,batch_size=batch_size,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recall that the images have 3 channels and are 32x32. The batch size=64\n",
    "- What is the shape of **imgs** and **labels** in the code below?\n",
    "- Use Vevox session 131-699-292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "print(imgs.size(),labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4udcJFDWf8-8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "- Logistic Regression can be regarded as the **simplest neural network**, a single \"neuron\". \n",
    "- It takes as input a vector of size $n$ and it feeds a single unit (a neuron or perceptron). \n",
    "- The neuron is represented by a vector of **learnable** weights $w$ and bias $b$\n",
    "- The output is the sum of $b$ and the **dot** product between $w$ and the vector input $x$\n",
    "- The result is fed into some function (usually nonlinear) $f$ called the activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z&=\\sum_iw_i\\cdot x_i+b\\\\\n",
    "\\hat{y}(x)&=f(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Note that because $z$ depends on $W$ and $b$ so does $\\hat{y}$. \n",
    "- Therefore our task is to **learn** the \"best\" values of $W$ and $b$ to model the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The input $x$ and $f$ are known whereas $w$ and $b$ are parameters to be determined. \n",
    "- Our goal is to find the _optimal_ $w$ and $b$ such that the output is as *close as possible* to the label associated with the input.\n",
    "![title](https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/perceptron.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- How is \"**as close as possible**\" defined? \n",
    "- The dataset is usually a set of pairs $(x,y)$. \n",
    "- We define the loss as the **deviation** between the label $y$ and the result $\\hat{y}=f(z)$\n",
    "\n",
    "$$loss=\\mathcal{L}_{w,b}(y,\\hat{y})$$\n",
    "\n",
    "- The function $\\mathcal{L}$ depends on the problem (for example binary cross entropy, mean squared error,...)\n",
    "\n",
    "- Note that $\\mathcal{L}$ depends on the parameters $w,b$. \n",
    "- Our goal is to find the **optimal** $w,b$ such that $\\mathcal{L}$ is minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wha5x_KeRHkw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sigmoid\n",
    "\n",
    "- So far we have not specified the function _f_ that our  model depends on $\\hat{y}=f(z)$. \n",
    "- In this example we use the **sigmoid** function. Given an input _z_ it has the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma=\\frac{1}{1+e^{-z}}\n",
    "\\end{align*}\n",
    "$$\n",
    "- The values of $\\sigma$ go from 0 to 1 which we interpret as the probability that the label is 1\n",
    "- $1-\\sigma$ is the probability that the label is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "wAe6yJrBRN_1",
    "outputId": "7bd84e2d-f3f9-44e0-f6b9-d6565aaff745",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "z=[1/(1+np.exp(-x)) for x in range(-10,11)]\n",
    "plt.plot([x for x in range(-10,11)],z)\n",
    "plt.xticks([t for t in range(-10,11,2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vMc-jV9pIcd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flatening the images\n",
    "- The input images have dimensions (3,32,32) (3 channels, 32 height,32 width). \n",
    "- To use them as input to the \"neuron\" we need to \"flatten\" the input\n",
    "- One can use the ```.reshape``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvukYwIicPqx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "imgs=imgs.reshape(batch_size,-1)\n",
    "print(imgs.shape,labels.shape)\n",
    "torch.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ```reshape``` does not work when the size of the datasets is **not** a multiple of the batch size\n",
    "- For CIFAR10 there are 50000 training samples. \n",
    "- That's 781 batches of size 64 and the last has size 16: ```50000=781*64+16```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    imgs,labels=batch\n",
    "    \n",
    "print(imgs.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### use of ```flatten```\n",
    "\n",
    "- It is more convenient to use ```tensor.flatten(start_dim=d)```\n",
    "- Where ```d``` specifies from which dimension to start \"flattening\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(imgs.shape)\n",
    "print(imgs.flatten(start_dim=0).shape,imgs.flatten(start_dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-VxiVGnp2LC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize the parameters\n",
    "- The goal is to find the **optimal** values for the parameters, $w$ and $b$. \n",
    "- Intially we give them random values (for weights) and 0 for the bias as shown below. \n",
    "- Note that\n",
    "    - The `reguires_grad` declares a tensor to be a variable\n",
    "    - In previous versions of Pytorch one needed to declare variables explicitly but this is deprecated now. See [here](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koYSysX4gKjr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weights=torch.randn(3*32*32,requires_grad=True,dtype=torch.float32)\n",
    "bias=torch.tensor(0.,requires_grad=True,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z9ssRJxX6iw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rate=0.015\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=0\n",
    "    count=0\n",
    "    for imgs,labels in train_loader:\n",
    "        count+=1\n",
    "        imgs=imgs.flatten(start_dim=1)\n",
    "        y_hat=torch.matmul(imgs,weights)+bias\n",
    "        y_hat=torch.sigmoid(y_hat)\n",
    "        loss=loss_fn(y_hat,labels.float())\n",
    "        dw,db=torch.autograd.grad(loss,[weights,bias])\n",
    "        #update the weights and bias\n",
    "        weights.data-=rate*dw\n",
    "        bias.data-=rate*db\n",
    "        epoch_loss+=loss.item()\n",
    "  \n",
    "    print(\"loss {:.4f}\".format(epoch_loss/count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJGEPlrz5l0U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction on the test data\n",
    "\n",
    "- An important measure of any ML method is how well it \"generalizes\". \n",
    "- This is done by using the trained model on **test** data, i.e. data that it **was not** trained on \n",
    "- But he output of our model is the probability that the input is a \"machine\", which could be any value between 0 and 1. \n",
    "- The test labels are discrete values of 0 and 1 so how do we compare them? \n",
    "- We regard a probability $\\ge 0.5$ to be 1 and $< 0.5$ to be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XttSuF7xuVom",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(loader):\n",
    "    total=0.\n",
    "    for imgs,labels in loader:\n",
    "        imgs=imgs.flatten(start_dim=1)\n",
    "        y_hat=torch.matmul(imgs,weights)+bias\n",
    "        y_hat=torch.sigmoid(y_hat)\n",
    "        ones=y_hat>0.5\n",
    "        ## count how many outputs are equal to the \"true\" labels\n",
    "        r=ones==labels\n",
    "        ## add them to the total\n",
    "        total+=r.sum()\n",
    "    \n",
    "    return total/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdCBzLnqc6tO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Abstracting the model and training pipeline using Pytorch\n",
    "\n",
    "- The model we have used  is simple enough to code directly. \n",
    "- We only needed Pytorch to compute the loss and gradients. \n",
    "- For more complicated models this process becomes unwieldy. \n",
    "- We can use Pytorch to abstract away the details.  \n",
    "- The abstractions offered by Pytorch are illustrated below to solve the same problem that we just did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1baF5ll16Q3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The model\n",
    "\n",
    "- The model we plan to use is encapsulated in a class that **inherits** from ```torch.nn.Module```\n",
    "\n",
    "- All we need to do is **override** two methods:\n",
    "1. ```__init__```. As you would have guessed this is called when the object is constructed to initialize our model\n",
    "1. ``` forward```. This is called to perform a forward computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LlED-xrgbiJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,in_features,out_features):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size=in_features\n",
    "    self.output_size=out_features\n",
    "    # declaring weights and bias as parameters so that they are included\n",
    "    # in the return value of .parameters()\n",
    "    self.weights=nn.Parameter(torch.randn(in_features,requires_grad=True,dtype=torch.float32))\n",
    "    self.bias=nn.Parameter(torch.tensor(0.,requires_grad=True,dtype=torch.float32))\n",
    "    #self.layer=nn.Linear(self.input_size,self.output_size,bias=True)\n",
    "  def forward(self,input):  \n",
    "    y_hat=input.flatten(start_dim=1)\n",
    "    y_hat=torch.matmul(y_hat,self.weights)+self.bias\n",
    "    y_hat=torch.sigmoid(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm160c2I23J3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that in the initialization, the weights and bias are constructed as ```Parameter```. \n",
    "- This is so that we can use the ```.parameters()``` call and pass it to the optimizer.\n",
    "- Next we create an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9_n6Y2N00Io",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model=Net(3*32*32,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKkSOrAn35u5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall that each learning iteration performs a number of steps. \n",
    "1. Compute the forward pass over the input to get the output. This is now done using ```model.forward()``` indirectly by calling ```model(input)```\n",
    "1. Compute the loss using an appropriate loss function. Same as before\n",
    "1. Compute the gradients using ```loss.backward()```.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ```backward()``` computes the gradient with respect to the parameters AND saves them in the ```.grad``` attributes\n",
    "- For example, if ```p``` is a parameters then ```loss.backward()``` computes the gradient of ```loss``` wrt ```p``` AND saves the result in ```p.grad```\n",
    "- Once the gradients are computed\n",
    "1. The optimizer updates the parameters. \n",
    "    - This is done by the optimizer using ```optimizer.step()```. \n",
    "1. This is important since later on we will use optimizers that use a different strategy to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8L9FtlGbz1S3",
    "outputId": "a8377b80-f564-4d1b-dfef-eeabdfdeeaeb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rate=0.015\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(model.parameters(),lr=rate)\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "epochs=20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss=0\n",
    "  count=0\n",
    "  \n",
    "  for imgs,labels in train_loader:\n",
    "    count+=1\n",
    "  # uses the .forward() method to get y_hat\n",
    "    y_hat=model(imgs)\n",
    "  # as before\n",
    "    loss=loss_fn(y_hat,labels.float())\n",
    "  # Computes the gradients and saves them in the appropriate .grad\n",
    "    loss.backward()\n",
    "  # updates the parameters using the computed .grad\n",
    "    optimizer.step()\n",
    "  # zero the .grad values so that they don't accumulate\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss+=loss.item()\n",
    "  print(\"loss {:.4f}\".format(epoch_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def predict(loader):\n",
    "    total=0.\n",
    "    for imgs,labels in loader:\n",
    "        outputs=model(imgs)\n",
    "        ones=outputs>0.5\n",
    "        r=ones==labels\n",
    "        total+=r.sum()\n",
    "    # Compute vector \"y_hat\" predicting\n",
    "    # the probabilities of a machine being present in the picture\n",
    "    \n",
    "    return total/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyNuNqO6ce7qryqS+HxdL/2J",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10a2f8fa1176415eb1427954d02ac81e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fe3e0a792204b568fdeada38bff559a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "716eed7ac5b242b1a43d095c0376fdfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eff7dac6203a4409bd25e5a0fafbfe5f",
      "placeholder": "​",
      "style": "IPY_MODEL_ffe956b562a148a9abb63f496ce91030",
      "value": " 170498071/170498071 [00:02&lt;00:00, 73485295.02it/s]"
     }
    },
    "9daf573e127d45b0a13d392d625f6dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df2a9b2f02d2434682d4ca213568fbc9",
       "IPY_MODEL_ffce6ae3a0f14491a4690ddec0df4674",
       "IPY_MODEL_716eed7ac5b242b1a43d095c0376fdfd"
      ],
      "layout": "IPY_MODEL_10a2f8fa1176415eb1427954d02ac81e"
     }
    },
    "c25206d0d8be44e3b18028c61228dbd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c49c0f939d094f8bb00e028bcae134cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df2a9b2f02d2434682d4ca213568fbc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc2f840078b44934a9a0a972b2d1a6aa",
      "placeholder": "​",
      "style": "IPY_MODEL_c49c0f939d094f8bb00e028bcae134cb",
      "value": "100%"
     }
    },
    "eff7dac6203a4409bd25e5a0fafbfe5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc2f840078b44934a9a0a972b2d1a6aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffce6ae3a0f14491a4690ddec0df4674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c25206d0d8be44e3b18028c61228dbd4",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fe3e0a792204b568fdeada38bff559a",
      "value": 170498071
     }
    },
    "ffe956b562a148a9abb63f496ce91030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
