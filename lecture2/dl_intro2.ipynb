{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/pytorch/blob/main/dl_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXG2Gc34Vqio"
   },
   "source": [
    "# What you will learn in this notebook\n",
    " \n",
    "1. Introduction to supervised learning and classification\n",
    "1. Introduction to loss functions, activation functions, and gradient descent\n",
    "1. Introduction to Pytorch packages, tensors, computation graphs, and gradients\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zpJEiPpbHjjk"
   },
   "source": [
    "## CIFAR10 Dataset\n",
    "\n",
    "- We have already seen the CIFAR10 dataset\n",
    "- We will use it to do binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4j6mq7QsZzsF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision as vision\n",
    "\n",
    "cifar10_train=vision.datasets.CIFAR10(\".\",download=True,train=True)# train=True is the default\n",
    "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m7zBqVB3oCeV"
   },
   "source": [
    "### Data transforms\n",
    "- Recall from last session that CIFAR10 contains a set of images/labels\n",
    "- To use the dataset with PyTorch we need to **transform** the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JziiKBlKnDqH",
    "outputId": "e3603c30-1504-430c-b021-6e9732b0d4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# create an iterator to the dataset\n",
    "itr=iter(cifar10_train)\n",
    "img0,label0=next(itr)\n",
    "print(type(img0),type(label0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "cifar10_train=vision.datasets.CIFAR10(\".\",download=True,train=True,transform=ToTensor())\n",
    "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False,transform=ToTensor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "\n",
    "- The dataset has 10 classes: Airplanes,Cars,Birds,Cats,Deers,Dogs,Frogs,Horses, Ships and Trucks\n",
    "- For simplicity we will rearrange it into only 2 classes\n",
    "    - \"Living things\"\n",
    "    - \"Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airplane=0,car=1,bird=2,cat=3,deer=4,dog=5,frog=6,horse=7,ship=8,truck=9\n",
    "features=torch.tensor([0,1,8,9])\n",
    "for i, (img,label) in enumerate(cifar10_train):\n",
    "    if torch.isin(label,features):\n",
    "        cifar10_train.targets[i]=1\n",
    "    else:\n",
    "        cifar10_train.targets[i]=0\n",
    "for i, (img,label) in enumerate(cifar10_test):\n",
    "    if torch.isin(label,features):\n",
    "        cifar10_test.targets[i]=1\n",
    "    else:\n",
    "        cifar10_test.targets[i]=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "- So far the gradient was computed using the whole dataset\n",
    "- In many situations this is not feasible\n",
    "- A good approximation is stochastic gradient descent\n",
    "- The gradient is computed for a single sample\n",
    "- Most commonly the gradient is computed over a batch (or mini-batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "- Mini-batch gradient descent is more efficient when the batches are randomly selected\n",
    "- PyTorch provides a convenient class for operations on batches: ```DataLoader```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=32\n",
    "train_loader=DataLoader(cifar10_train,batch_size=batch_size,num_workers=2)\n",
    "test_loader=DataLoader(cifar10_test,batch_size=batch_size,num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.int64\n",
      "torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "print(type(imgs),labels.dtype)\n",
    "print(imgs.shape,labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4udcJFDWf8-8"
   },
   "source": [
    "\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "- Logistic Regression which can be regarded as the **simplest neural network**, a single \"neuron\". \n",
    "\n",
    "As can be seen from the figure below the input is a vector of size _n_ and it feeds a single unit (a neuron or perceptron). To obtain the output we perform the **dot** product between the matrix **W** and the input **x** and the result is fed into some function (usually nonlinear) _f_\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z&=\\sum_iw_i\\cdot x_i+b\\\\\n",
    "\\hat{y}(x)&=f(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $z$ depends on $w$ and $b$ so does $\\hat{y}$. The input and _f_ are known whereas _W_ and _b_ are parameters to be determined. Our goal is to find the _optimal_ _W_ and _b_ such that the output is as *close as possible* to the label associated with the input.\n",
    "![title](https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/perceptron.png?raw=1)\n",
    "\n",
    "How is **as close as possible** defined? The dataset is usually a set of pairs $(x,y)$. We define the loss as the **deviation** between the label $y$ and the result $\\hat{y}=f(z)$\n",
    "\n",
    "$$loss=E_{w,b}(y,\\hat{y})$$\n",
    "\n",
    "The function $E$ depends on the problem (for example binary cross entropy, mean squared error,...)\n",
    "\n",
    "Note that $E$ depends on the parameters $w,b$. Our goal is to find the **optimal** $w,b$ such that the loss is minimal. From calculus we know that to find the minimum (max) of a function we compute its derivative and find where it is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wha5x_KeRHkw"
   },
   "source": [
    "## Sigmoid\n",
    "\n",
    "So far we have not specified the function _f_ that our  model depends on $\\hat{y}=f(z)$. In this example we use the **sigmoid** function. Given an input _z_ it has the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma=\\frac{1}{1+e^{-z}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can plot the sigmoid function using matplotlib. As you can see below the values of $\\sigma$ go from 0 to 1 which we interpret as a probability. For example, if $\\sigma=0.65$ then the probability of the image being a **ship** is 0.65 and of **not** being one is 0.35 so decide it is a ship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "wAe6yJrBRN_1",
    "outputId": "7bd84e2d-f3f9-44e0-f6b9-d6565aaff745"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcZklEQVR4nO3de3zU9Z3v8dcnmVwkhIskoJJgQMFCxaKk1NparYhV6BG7bRV3tbW1sqdbe2z1dNdue9xdu499bNvTantqT0u1XtpVH9BalyN0vVRt7UUkeOGqELklgBBAbsGQTOZz/pgfOIRABjIzv5nfvJ+PRx7zu3yT72eSmTc/PjO/+Zm7IyIiha8k7AJERCQzFOgiIhGhQBcRiQgFuohIRCjQRUQiIhbWxDU1Nd7Q0BDW9CIiBWnJkiXb3b22t32hBXpDQwNNTU1hTS8iUpDMbMPR9qnlIiISEQp0EZGIUKCLiESEAl1EJCIU6CIiEdFnoJvZz81sm5ktP8p+M7MfmlmzmS01s/MyX6aIiPQlnSP0B4DLj7H/CmBs8DUb+L/9L0tERI5Xn+9Dd/c/mFnDMYbMBB7y5OfwvmhmQ8zsVHffkqkiRSSa4t0JDsQTxLudrkRw252gqztBPJFcjnc78USCznjy9uCYeMKJJxx3xx0S7iSCW09ZTjhHjEnuT247+AHiyWU/tJy89ZR9R44jZdth6z3vaI8BU8eP4H31Q/r1u+tNJk4sGgm0pKy3BtuOCHQzm03yKJ5Ro0ZlYGoRCVP7gTgbd+5n9ztd7OuIs+9AnL0H4sFyF3s7ksvvbgvGBPs7uhJh34WcMXt3efigyrwN9LS5+xxgDkBjY6OurCFSADq6utm4cz/rtrezbns767e3sza43bb3wFG/r7TEqK6MMbAi+VVdGaNmYDkNNVWH1qvKY1SWlRArLaGs1IiVlBArNcpLk7exkmB7sL+stIRYSXBbapSaUVJilJhRYlBihgW3B7fZYduS64e2kwza5NLhoXtw+8FtybGWsnz4/bWeG0KQiUDfBNSnrNcF20SkQMS7E7S+/c6h0F63vZ31O9pZ29bO5t3vHNYxqBlYTsOwKi4aV0tDTRUNw6oYWlVGdUUZAyvfDe+KWElehFwxyUSgzwduNrNHgQ8Au9U/F8l/Xd0Jnnt9G3ObWvn96m10db+b2tWVMcbUVNHYMJTRNXWMrqlidE0VDTVVDKosC7FqOZY+A93MHgEuBmrMrBX4J6AMwN1/AiwEpgPNwH7gc9kqVkT6b83Wvcxb0spjL7eyfV8ntdUVXH9+A+85tZoxQXCfXFWuo+sClM67XK7tY78DX8pYRSKScXs6unjitS3MbWrh1ZZdxEqMqeOHc3VjPReNqyVWqnMMoyC0j88VkexKJJxF63Yyr6mFhcu30NGVYNyIgXxzxniuOnckNQMrwi5RMkyBLhIxm3e9w6+XtDJvSSsbd+6nuiLGJ8+r4+rGes6pG6xWSoQp0EUioDOe4KmVbzG3qZUX1rThDhecMYxbp43jY+89hZPKS8MuUXJAgS5S4N5u7+SGBxbzWssuThtcyZcvGcunJ9dRf/KAsEuTHFOgixSwrXs6uP6+RazfsZ8fzJrEx885jdIStVSKlQJdpEBt2NHOdfctYue+Th743Pu54IyasEuSkCnQRQrQ62/t4fr7XqKrO8HDN52flc8FkcKjQBcpMC9vfJvP3b+YyrIS5v3tBxk7ojrskiRPKNBFCsgf12xn9i+aqK2u4Jc3fkAvfMphFOgiBeK/lm/hfzzyKmNqq3joxikMr64MuyTJMwp0kQIwt6mF23+9lEn1Q7j/hikMHqAPyJIjKdBF8ty9L6zlXxes4sKxNfz0+skMKNfTVnqnR4ZInnJ3vv/0av7Ps81Mn3gKd10ziYqYzviUo1Ogi+ShRML5l/+3ggf/soFrGuv5t7+aqBOGpE8KdJE809Wd4GvzXuPxVzdz04Wj+cfp4/WBWpIWBbpIHuno6ubmh1/mmVXb+NrHzuLvLj5DYS5pU6CL5Im9HV184cEmXlq/k29ddTbXn3962CVJgVGgi+QBd+emh5pYsuFt7r5mEjMnjQy7JClAuu6USB547o1tvLh2J//03yYozOWEKdBFQubufO+p1Yw6eQCzpowKuxwpYAp0kZA9ueItVmzewy1Tx1KmizVLP+jRIxKi7kTy5KEzaqu46ly1WqR/FOgiIXpi6WZWb93HV6eN04lD0m8KdJGQxLsT3P3MGt5zSjXTzz417HIkAhToIiF57JVNrNvezq3TxlGio3PJAAW6SAg64wl++Ls1nFM3mGkTRoRdjkSEAl0kBHObWmh9+x1unTZOp/ZLxijQRXKso6ubHz3bTOPpQ7loXG3Y5UiEKNBFcuzhRRt5a08Ht16mo3PJLAW6SA7t74zz4+ebueCMYVxwRk3Y5UjEKNBFcuihv2xg+75ObrtsXNilSASlFehmdrmZvWFmzWZ2ey/7R5nZc2b2ipktNbPpmS9VpLDt7ejiJ79/k4vPqmXy6SeHXY5EUJ+BbmalwD3AFcAE4Fozm9Bj2DeBue5+LjAL+HGmCxUpdD//43p27e/itmlnhV2KRFQ6R+hTgGZ3X+vuncCjwMweYxwYFCwPBjZnrkSRwrdrfyf3vrCWyyaMYGLd4LDLkYhKJ9BHAi0p663BtlT/DFxnZq3AQuDLvf0gM5ttZk1m1tTW1nYC5YoUpp+9sJZ9nXFuVe9csihTL4peCzzg7nXAdOAXZnbEz3b3Oe7e6O6NtbV6/60Uhx37DnD/n9YzY+KpvOeUQX1/g8gJSifQNwH1Ket1wbZUNwJzAdz9L0AloPdkiQA/+f2bdHR185VLdXQu2ZVOoC8GxprZaDMrJ/mi5/weYzYCUwHMbDzJQFdPRYre1j0dPPSXDXzi3DrOHD4w7HIk4voMdHePAzcDTwKrSL6bZYWZ3WlmVwbDbgNuMrPXgEeAG9zds1W0SKH48XPNdCecW6aODbsUKQKxdAa5+0KSL3ambrsjZXkl8KHMliZS2DbteodHXmrh0431jBo2IOxypAjoTFGRLPnRs2sA+PIlZ4ZciRQLBbpIFqzf3s7cplb++gOjOG3ISWGXI0VCgS6SBT/83RrKSo2/u/iMsEuRIqJAF8mw5m17efzVTXzmgw0MH1QZdjlSRBToIhl21zNrOKmslL/9yJiwS5Eio0AXyaCVm/ewYOkWPveh0QwbWBF2OVJkFOgiGXTXM6uproxx04U6OpfcU6CLZMiGHe08vXIrN354NIMHlIVdjhQhBbpIhixYtgWAT02uC7kSKVYKdJEMWbhsC++rH0LdUJ0VKuFQoItkwIYd7SzftIcZE08JuxQpYgp0kQw42G6ZPvHUkCuRYqZAF8kAtVskHyjQRfpJ7RbJFwp0kX5Su0XyhQJdpJ/UbpF8oUAX6YeNO/ar3SJ5Q4Eu0g8H2y1XnK12i4RPgS7SDwuWbeZ99UOoP1ntFgmfAl3kBKndIvlGgS5ygtRukXyjQBc5QWq3SL5RoIucALVbJB8p0EVOgNotko8U6CInYOGyLbyvbrDaLZJXFOgix2njjv0s27SbGefo6FzyiwJd5Dip3SL5SoEucpzUbpF8pUAXOQ5qt0g+U6CLHAe1WySfpRXoZna5mb1hZs1mdvtRxlxtZivNbIWZPZzZMkXyg9otks/6DHQzKwXuAa4AJgDXmtmEHmPGAl8HPuTu7wW+kvlSRcJ1sN2iC1lIvkrnCH0K0Ozua929E3gUmNljzE3APe7+NoC7b8tsmSLh05WJJN+lE+gjgZaU9dZgW6pxwDgz+5OZvWhml/f2g8xstpk1mVlTW1vbiVUsEhK1WyTfZepF0RgwFrgYuBb4mZkN6TnI3ee4e6O7N9bW1mZoapHsU7tFCkE6gb4JqE9Zrwu2pWoF5rt7l7uvA1aTDHiRSFC7RQpBOoG+GBhrZqPNrByYBczvMeZxkkfnmFkNyRbM2syVKRIutVukEPQZ6O4eB24GngRWAXPdfYWZ3WlmVwbDngR2mNlK4Dnga+6+I1tFi+SS2i1SKGLpDHL3hcDCHtvuSFl24NbgSyRSFi5Xu0UKg84UFenDgqVqt0hhUKCLHIPaLVJIFOgix6B2ixQSBbrIMajdIoVEgS5yFGq3SKFRoIschdotUmgU6CJHsWDpFs5Ru0UKiAJdpBeHrkyko3MpIAp0kV6o3SKFSIEu0ouFy9RukcKjQBfpYeOO/SxtVbtFCo8CXaQHtVukUCnQRXpQu0UKlQJdJEXLTrVbpHAp0EVS6MpEUsgU6CIp1G6RQqZAFwkcbLfo6FwKlQJdJHCw3aL+uRQqBbpIQO0WKXQKdBHUbpFoUKCLoHaLRIMCXQS1WyQaFOhS9NRukahQoEvRW6h2i0SEAl2K3gK1WyQiFOhS1NRukShRoEtRU7tFokSBLkVN7RaJEgW6FC21WyRqFOhStNRukahRoEvRWrhsCxNHqt0i0ZFWoJvZ5Wb2hpk1m9ntxxj3STNzM2vMXIkimdeycz+vte5mxjk6Opfo6DPQzawUuAe4ApgAXGtmE3oZVw3cAizKdJEimaZ2i0RROkfoU4Bmd1/r7p3Ao8DMXsZ9C/g20JHB+kSyQu0WiaJ0An0k0JKy3hpsO8TMzgPq3X3BsX6Qmc02syYza2prazvuYkUyQe0Wiap+vyhqZiXA94Hb+hrr7nPcvdHdG2tra/s7tcgJUbtFoiqdQN8E1Kes1wXbDqoGzgaeN7P1wPnAfL0wKvlK7RaJqnQCfTEw1sxGm1k5MAuYf3Cnu+929xp3b3D3BuBF4Ep3b8pKxSL9cLDdopOJJIr6DHR3jwM3A08Cq4C57r7CzO40syuzXaBIJqndIlEWS2eQuy8EFvbYdsdRxl7c/7JEsuNgu2XUMLVbJHp0pqgUDbVbJOoU6FI01G6RqFOgS9FQu0WiToEuRUHtFikGCnQpCmq3SDFQoEtRULtFioECXSJP7RYpFgp0ibzfLle7RYqDAl0ib8FStVukOCjQJdLUbpFiokCXSFO7RYqJAl0iTe0WKSYKdIkstVuk2CjQJbLUbpFio0CXyFqw7C3OHjlI7RYpGgp0iaSWnft5rWUXMyaeFnYpIjmjQJdIUrtFipECXSJJ7RYpRgp0iRy1W6RYKdAlcua/thmA6RNPCbkSkdxSoEuk7O3o4t4X1nLh2BpOH1YVdjkiOaVAl0i5/0/reXt/F7dddlbYpYjknAJdImP3/i5+9sJaLh0/gkn1Q8IuRyTnFOgSGT97YS17O+LcOm1c2KWIhEKBLpGwY98B7v/TOmZMPJUJpw0KuxyRUCjQJRJ++oe1vNPVzVenjQ27FJHQKNCl4G3b08GDf17PzEkjOXN4ddjliIRGgS4F78fPv0k84dwyVUfnUtwU6FLQNu16h4cXbeTTk+toqNH7zqW4KdCloP3o2WYc5+ZLzgy7FJHQpRXoZna5mb1hZs1mdnsv+281s5VmttTMfmdmp2e+VJHDbdjRzrymFq6dMoq6ofoQLpE+A93MSoF7gCuACcC1Zjahx7BXgEZ3Pwf4FfCdTBcq0tMPfreG0hLjSx/V0bkIpHeEPgVodve17t4JPArMTB3g7s+5+/5g9UWgLrNlihyueds+Hn9lE9effzojBlWGXY5IXkgn0EcCLSnrrcG2o7kR+G1vO8xstpk1mVlTW1tb+lWK9HD3M6upLCvlv198RtiliOSNjL4oambXAY3Ad3vb7+5z3L3R3Rtra2szObUUkVVb9vDE0i3ccEEDNQMrwi5HJG/E0hizCahPWa8Lth3GzC4FvgFc5O4HMlOeyJHueno11RUxZn9kTNiliOSVdI7QFwNjzWy0mZUDs4D5qQPM7Fzgp8CV7r4t82WKJC1t3cVTK7fyhQvHMGRAedjliOSVPgPd3ePAzcCTwCpgrruvMLM7zezKYNh3gYHAPDN71czmH+XHifTL959ezZABZXz+ww1hlyKSd9JpueDuC4GFPbbdkbJ8aYbrEjnCkg07ef6NNv7h8vdQXVkWdjkieUdnikrB+N5Tq6kZWM5nL9B5ayK9UaBLQfhz83b+/OYOvnjxmQwoT+s/liJFR4Euec/d+d7TqzllUCV/84FRYZcjkrcU6JL3fr+6jSUb3uZLl5xJZVlp2OWI5C0FuuQ1d+f7T69m5JCTuKaxvu9vECliCnTJa0+v3MrS1t3cMnUs5TE9XEWORc8QyVuJRPLovGHYAP7qvGN9fJCIgAJd8tiCZVt4/a29fHXaOGKleqiK9EXPEslL8e4Edz2zmrHDB/Lxc04LuxyRgqBAl7z0+KubWdvWzq3TxlFaYmGXI1IQFOiSd/64Zjt3/OdyJo4czMfee0rY5YgUDAW65JX/Wv4Wn39gMaNOHsB9n22kREfnImnTOdSSN+Y1tfAPv17K++qHcP8N79fH44ocJwW65IX7/riObz2xkgvH1vCT6yZTVaGHpsjx0rNGQuXu3PX0an74bDNXnH0Kd8+aREVMp/eLnAgFuoQmkXDufGIlD/x5PVc31vFvn5io95uL9IMCXULR1Z3g73+1lN+8sokvfHg035gxHjO9ACrSHwp0ybmOrm5ufvgVnlm1lf952Ti+9NEzFeYiGaBAl5zadyDOFx5czItrd3LnzPfymQ82hF2SSGQo0CVndrZ3csP9L7Fi8x7uvmYSV52rD9wSySQFuuTElt3vcP19L9Gycz9zrp/M1PEjwi5JJHIU6JJ167e38zf3LmL3O108+PkpnD9mWNgliUSSAl2yauXmPXzm5y/RnUjwyE3nM7FucNgliUSWAl2yZvH6ndz4wGKqKmI8OvuDnDm8OuySRCJNgS4Ztb8zzm+XvcXcphYWrdvJ6JoqfnHjFOqGDgi7NJHIU6BLv7k7L2/cxbymFp5YuoV9B+I0DBvA1z52Fn89ZRRDq/QhWyK5oECXE7Ztbwe/eXkTc5taeLOtnZPKSplxzqlc3VjP+xuG6mQhkRxToMtx6epO8Nzr25jb1Mpzb2yjO+FMPn0o3/7kGGaccxoD9SmJIqHRs0/SsmbrXuYtaeWxl1vZvq+T2uoKbrpwDJ+aXMeZwweGXZ6IoECXXrg7O9s7Wbe9nZVb9vCbVzbxysZdxEqMqeOHc3VjPReNq9UnI4rkGQV6EdvT0cX67e2sC74OLq/d3s7ejvihcWOHD+SbM8Zz1bkjqRlYEWLFInIsaQW6mV0O/AAoBe5193/vsb8CeAiYDOwArnH39ZktVY6Hu7O/s5t9B+LsbO9kw45kUK9ra2f9jmRwb9/XeWi8GZw2+CRG11Rx1aSRNNRUMaamitE1VZw+bIBe4BQpAH0GupmVAvcA04BWYLGZzXf3lSnDbgTedvczzWwW8G3gmmwUXKjcnXjCiXc7XYlE8rY7QVd3cjmeSNB1aJsT704QT7y7fiDezb6OOPsOxNkb3B5aPxBnX0fXoW17D8RpPxAn4UfWUVtdweiaKqa+ZwSja6toGFbFmNoqRp08gMoyXSlIpJClc4Q+BWh297UAZvYoMBNIDfSZwD8Hy78CfmRm5u69REr/zF3cwpwX1h5a7znFERP6kasHvye5fHC74/7u+sGf7YeW3x2T8OS+hDsJh0Ry52Hrfmjdew3W/qoqL2VgZYyBFTEGVpZRXRFjeHXloW3Vh/bFGHJSOacPG0BDTZXehSISYek8u0cCLSnrrcAHjjbG3eNmthsYBmxPHWRms4HZAKNGjTqhgodWlXPWiB6nkNsxV49oFxjJFsO7y/bu9xlY8BPMeo41SkqS40sMSswoCXaWHNxWYpilrJthlvyJsRIjVlpCWakRKzHKYiWUlZQQKw22lxhlpcn1stKSw8ZXxEqTIV0Zo6o8RmmJWiAicricHq65+xxgDkBjY+MJHbdOmzCCaRP00asiIj2l876zTUB9ynpdsK3XMWYWAwaTfHFURERyJJ1AXwyMNbPRZlYOzALm9xgzH/hssPwp4Nls9M9FROTo+my5BD3xm4EnSb5t8efuvsLM7gSa3H0+cB/wCzNrBnaSDH0REcmhtHro7r4QWNhj2x0pyx3ApzNbmoiIHA+duy0iEhEKdBGRiFCgi4hEhAJdRCQiLKx3F5pZG7DhBL+9hh5noeZQWHPrPkd/3jDn1n0unLlPd/fa3naEFuj9YWZN7t5YTHPrPkd/3jDn1n2OxtxquYiIRIQCXUQkIgo10OcU4dy6z9GfN8y5dZ8jMHdB9tBFRORIhXqELiIiPSjQRUQioqAC3cw+bWYrzCxhZo099n3dzJrN7A0z+1iW65hkZi+a2atm1mRmU7I5X4+5v2xmrwe/h+/kat5g7tvMzM2sJodzfje4v0vN7DdmNiTL810ePIaazez2bM6VMme9mT1nZiuDv+stuZi3Rw2lZvaKmT2RwzmHmNmvgr/vKjP7YA7n/mrwu15uZo+YWWWW5vm5mW0zs+Up2042s6fNbE1wOzRjE7p7wXwB44GzgOeBxpTtE4DXgApgNPAmUJrFOp4CrgiWpwPP5+j+fxR4BqgI1ofn8HdfT/IjlDcANTmc9zIgFix/G/h2FucqDR47Y4Dy4DE1IQf38VTgvGC5Glidi3l71HAr8DDwRA7nfBD4QrBcDgzJ0bwjgXXAScH6XOCGLM31EeA8YHnKtu8AtwfLt2fyMV1QR+juvsrd3+hl10zgUXc/4O7rgGaSF7fOWinAoGB5MLA5i3Ol+iLw7+5+AMDdt+VoXoC7gL+nl+twZ5O7P+Xu8WD1RZJXzMqWQxdEd/dO4OAF0bPK3be4+8vB8l5gFcnQyQkzqwNmAPfmcM7BJMPuPgB373T3Xbman+RHh58UXGFtAFl6Drv7H0heIyLVTJL/mBHcXpWp+Qoq0I+htwtZZ/MJ8RXgu2bWAvxv4OtZnCvVOOBCM1tkZr83s/fnYlIzmwlscvfXcjHfMXwe+G0Wf36uH0dHMLMG4FxgUQ6nvZvkP9aJHM45GmgD7g9aPfeaWVUuJnb3TSSftxuBLcBud38qF3MHRrj7lmD5LSBjF0nO6UWi02FmzwCn9LLrG+7+n/lQBzAV+Kq7/9rMriZ5lHFpDuaNAScD5wPvB+aa2RgP/u+WxXn/kWTrIyvS+Zub2TeAOPAf2aojbGY2EPg18BV335OjOT8ObHP3JWZ2cS7mDMRItiK+7O6LzOwHJNsP/yvbEwc965kk/1HZBcwzs+vc/ZfZnrsnd3czy9j/evMu0N39RIIxnQtZZ6wOM3sIOPjC1Twy+F/VPub9IvBYEOAvmVmC5If8tGVrXjObSPKB/5qZQfJ3+7KZTXH3t/o777HmTqnhBuDjwNRM/ON1DBl/HKXLzMpIhvl/uPtjuZgz8CHgSjObDlQCg8zsl+5+XZbnbQVa3f3g/0R+RTLQc+FSYJ27twGY2WPABUCuAn2rmZ3q7lvM7FQgY63TqLRc5gOzzKzCzEYDY4GXsjjfZuCiYPkSYE0W50r1OMkXRjGzcSRfSMrqp8W5+zJ3H+7uDe7eQPKJeF6mwrwvZnY5yXbAle6+P8vTpXNB9Iyz5L+U9wGr3P372Z4vlbt/3d3rgr/tLJIXeM92mBM8flrM7Kxg01RgZbbnDWwEzjezAcHvfirJ1y1yZT7w2WD5s0DmOg+5eFU5g68Yf4JkoBwAtgJPpuz7Bsl3KLxB8A6ULNbxYWAJyXdBLAIm5+j+l5M8ilgOvAxcEsLfYD25fZdLM8m+9qvB10+yPN90ku8yeZNkyycX9/HDJF9sXppyP6eH8Le9mNy+y2US0BTc78eBoTmc+1+A14Pn0i8I3jmWhXkeIdmn7wqy60ZgGPA7kgeCzwAnZ2o+nfovIhIRUWm5iIgUPQW6iEhEKNBFRCJCgS4iEhEKdBGRiFCgi4hEhAJdRCQi/j/8aTN5lJWfdQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "z=[1/(1+np.exp(-x)) for x in range(-10,11)]\n",
    "plt.plot([x for x in range(-10,11)],z)\n",
    "plt.xticks([t for t in range(-10,11,2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vMc-jV9pIcd"
   },
   "source": [
    "## Flatening the images\n",
    "The images have dimensions (3,32,32) (3 channels, 32 height,32 width). To feed them to our \"neuron\" we need to create a vector of dimension 3x32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UvukYwIicPqx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3072]) torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "imgs=imgs.reshape(batch_size,-1)\n",
    "print(imgs.shape,labels.shape)\n",
    "torch.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7QpEeoqppWW"
   },
   "source": [
    "As mentioned before, to simplify the problem, we group all \"machines\" together by giving them the label 1 and all living things together by giving them the label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-VxiVGnp2LC"
   },
   "source": [
    "## Initialize the parameters\n",
    "Our goal is to find the **optimal** values for the parameters, weights and bias. Intially we give them random values (for weights) and 0 for the bias as shown below. Note two things\n",
    "1. We divide the initial values of the weights by the total number of samples to minimize the possibility of divergence.\n",
    "1. The `reguires_grad` declares a tensor to be a variable, i.e. we need the derivative. In previous versions of Pytorch one needed to declare variables explicitly but this is deprecated now. See [here](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "koYSysX4gKjr"
   },
   "outputs": [],
   "source": [
    "weights=torch.randn(3*32*32,requires_grad=True,dtype=torch.float32)\n",
    "bias=torch.tensor(0.,requires_grad=True,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5975, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn=torch.nn.BCELoss()\n",
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "imgs=imgs.flatten(start_dim=1)\n",
    "y_hat=torch.matmul(imgs,weights)+bias\n",
    "y_hat=torch.sigmoid(y_hat)\n",
    "loss=loss_fn(y_hat,labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z9ssRJxX6iw"
   },
   "source": [
    "## Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.3057\n",
      "loss 2.5578\n",
      "loss 2.1809\n",
      "loss 1.9531\n",
      "loss 1.7920\n",
      "loss 1.6611\n",
      "loss 1.5507\n",
      "loss 1.4555\n",
      "loss 1.3734\n",
      "loss 1.3015\n"
     ]
    }
   ],
   "source": [
    "rate=0.015\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    for imgs,labels in train_loader:\n",
    "        imgs=imgs.flatten(start_dim=1)\n",
    "        y_hat=torch.matmul(imgs,weights)+bias\n",
    "        y_hat=torch.sigmoid(y_hat)\n",
    "        loss=loss_fn(y_hat,labels.float())\n",
    "        dw,db=torch.autograd.grad(loss,[weights,bias])\n",
    "        #update the weights and bias\n",
    "        weights.data-=rate*dw\n",
    "        bias.data-=rate*db\n",
    "\n",
    "  \n",
    "    print(\"loss {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJGEPlrz5l0U"
   },
   "source": [
    "## Prediction on the test data\n",
    "\n",
    "An important measure of any ML method is how well it \"generalizes\". This is done by using the trained model on test data, i.e. other than the data it was trained on. To do that we note that the output of our model is the probability that the input is a \"machine\", which could be any value between 0 and 1. The test labels are discrete values of 0 and 1 so how do we compare them? We regard a probability $\\ge 0.5$ to be 1 and $< 0.5$ to be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XttSuF7xuVom"
   },
   "outputs": [],
   "source": [
    "def predict(loader):\n",
    "    total=0.\n",
    "    for imgs,labels in loader:\n",
    "        imgs=imgs.flatten(start_dim=1)\n",
    "        y_hat=torch.matmul(imgs,weights)+bias\n",
    "        y_hat=torch.sigmoid(y_hat)\n",
    "        ones=y_hat>0.5\n",
    "        r=ones==labels\n",
    "        total+=r.sum()\n",
    "    # Compute vector \"y_hat\" predicting\n",
    "    # the probabilities of a machine being present in the picture\n",
    "    \n",
    "    return total/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7483)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdCBzLnqc6tO"
   },
   "source": [
    "## Abstracting the model and training pipeline using Pytorch\n",
    "\n",
    "- The model we have used  is simple enough to code directly. \n",
    "- We only needed Pytorch to compute the gradients. \n",
    "- When more complicated models are used this process becomes unwieldy. \n",
    "- We can use Pytorch to abstract away the details.  \n",
    "- The abstractions offered by Pytorch are illustrated below to solve the same problem that we just did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1baF5ll16Q3"
   },
   "source": [
    "### The model\n",
    "\n",
    "- The model we plan to use is encapsulated in a class that **inherits** from ```torch.nn.Module```\n",
    "\n",
    "- All we need to do is **override** two methods:\n",
    "1. ```__init__```. As you would have guess this is called when the object is constructed to initialize our model\n",
    "1. ``` forward```. This is called everytime a forward computation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5LlED-xrgbiJ"
   },
   "outputs": [],
   "source": [
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "batch_size=32\n",
    "train_loader=DataLoader(cifar10_train,batch_size=batch_size,num_workers=2)\n",
    "test_loader=DataLoader(cifar10_test,batch_size=batch_size,num_workers=2)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,in_features,out_features):\n",
    "    super(Net, self).__init__()\n",
    "    self.input_size=in_features\n",
    "    self.output_size=out_features\n",
    "    # declaring weights and bias as parameters so that they are included\n",
    "    # in the return value of .parameters()\n",
    "    self.weights=nn.Parameter(torch.randn(in_features,requires_grad=True,dtype=torch.float32))\n",
    "    #self.weights.data/=in_features\n",
    "    self.bias=nn.Parameter(torch.tensor(0.,requires_grad=True,dtype=torch.float32))\n",
    "    #self.layer=nn.Linear(self.input_size,self.output_size,bias=True)\n",
    "  def forward(self,input):  \n",
    "    y_hat=input.flatten(start_dim=1)\n",
    "    y_hat=torch.matmul(y_hat,self.weights)+self.bias\n",
    "    y_hat=torch.sigmoid(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm160c2I23J3"
   },
   "source": [
    "- Note that in the initialization, the weights and bias are constructed as ```Parameter```. \n",
    "- This is so that we can use the ```.parameters()``` call and pass it to the optimizer.\n",
    "- Next we create an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Y9_n6Y2N00Io"
   },
   "outputs": [],
   "source": [
    "model=Net(3*32*32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8617, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "itr=iter(train_loader)\n",
    "imgs,labels=next(itr)\n",
    "outputs=model(imgs)\n",
    "loss=loss_fn(outputs,labels.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKkSOrAn35u5"
   },
   "source": [
    "Recall that each learning iteration performs a number of steps. \n",
    "1. Compute the forward pass over the input to get the output. This is now done using ```model.forward()``` indirectly by calling ```model(input)```\n",
    "1. Compute the loss using an appropriate loss function. Same as before\n",
    "1. Compute the gradients. Now we use ```loss.backward()```.\n",
    "    - Not only it computes the gradient with respect to the parameters but saves those values in the parameters themselves. \n",
    "    - For example, if ```p``` is a parameters then ```loss.backward()``` computes the gradient and saves it in ```p.grad```\n",
    "1. Update the parameters. This is done by the optimizer using ```optimizer.step()```. This is important since later on we will use optimizers that use a different strategy to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8L9FtlGbz1S3",
    "outputId": "a8377b80-f564-4d1b-dfef-eeabdfdeeaeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.9296\n",
      "loss 1.4413\n",
      "loss 1.2445\n",
      "loss 1.1275\n",
      "loss 1.0413\n",
      "loss 0.9792\n",
      "loss 0.9396\n",
      "loss 0.9155\n",
      "loss 0.8993\n",
      "loss 0.8869\n"
     ]
    }
   ],
   "source": [
    "rate=0.015\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(model.parameters(),lr=rate)\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "  for imgs,labels in train_loader:\n",
    "  # uses the .forward() method to get y_hat\n",
    "    y_hat=model(imgs)\n",
    "  # as before\n",
    "    loss=loss_fn(y_hat,labels.float())\n",
    "  # Computes the gradients and saves them in the appropriate .grad\n",
    "    loss.backward()\n",
    "  # updates the parameters using the computed .grad\n",
    "    optimizer.step()\n",
    "  # zero the .grad values so that they don't accumulate\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "  print(\"loss {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader):\n",
    "    total=0.\n",
    "    for imgs,labels in loader:\n",
    "        outputs=model(imgs)\n",
    "        ones=outputs>0.5\n",
    "        r=ones==labels\n",
    "        total+=r.sum()\n",
    "    # Compute vector \"y_hat\" predicting\n",
    "    # the probabilities of a machine being present in the picture\n",
    "    \n",
    "    return total/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7566)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuNqO6ce7qryqS+HxdL/2J",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10a2f8fa1176415eb1427954d02ac81e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fe3e0a792204b568fdeada38bff559a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "716eed7ac5b242b1a43d095c0376fdfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eff7dac6203a4409bd25e5a0fafbfe5f",
      "placeholder": "​",
      "style": "IPY_MODEL_ffe956b562a148a9abb63f496ce91030",
      "value": " 170498071/170498071 [00:02&lt;00:00, 73485295.02it/s]"
     }
    },
    "9daf573e127d45b0a13d392d625f6dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df2a9b2f02d2434682d4ca213568fbc9",
       "IPY_MODEL_ffce6ae3a0f14491a4690ddec0df4674",
       "IPY_MODEL_716eed7ac5b242b1a43d095c0376fdfd"
      ],
      "layout": "IPY_MODEL_10a2f8fa1176415eb1427954d02ac81e"
     }
    },
    "c25206d0d8be44e3b18028c61228dbd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c49c0f939d094f8bb00e028bcae134cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df2a9b2f02d2434682d4ca213568fbc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc2f840078b44934a9a0a972b2d1a6aa",
      "placeholder": "​",
      "style": "IPY_MODEL_c49c0f939d094f8bb00e028bcae134cb",
      "value": "100%"
     }
    },
    "eff7dac6203a4409bd25e5a0fafbfe5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc2f840078b44934a9a0a972b2d1a6aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffce6ae3a0f14491a4690ddec0df4674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c25206d0d8be44e3b18028c61228dbd4",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fe3e0a792204b568fdeada38bff559a",
      "value": 170498071
     }
    },
    "ffe956b562a148a9abb63f496ce91030": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
